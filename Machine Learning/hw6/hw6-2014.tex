%\input{../../cs446.tex}
\input{cs446.tex}
\usepackage{amsmath,url,graphicx,amssymb}
\sloppy
\usepackage{ulem}
\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in
\newcommand{\ignore}[1]{}

\newcommand{\tight}[1]{\!#1\!}
\newcommand{\loose}[1]{\;#1\;}

\begin{document}

\assignment{Fall 2014}{6}{November $6^{th}$, $2014$}{November $18^{th}$, $2014$}

\begin{footnotesize}
\begin{itemize}
\item Feel free to talk to other members of the class in doing the homework.  I am
more concerned that you learn how to solve the problem than that you
demonstrate that you solved it entirely on your own.  You should, however,
write down your solution yourself.  Please try to keep the solution brief and
clear.

\item Please use Piazza first if you have questions about the homework.
  Also feel free to send us e-mails and come to office hours.

\item Please, no handwritten solutions. You will submit your solution manuscript as a single pdf file.


\item The homework is due at 11:59 PM on the due date. We will be using
Compass for collecting the homework assignments. Please submit your solution manuscript as a pdf file via Compass 
(\texttt{http://compass2g.illinois.edu}). Please do NOT hand in a hard copy of your write-up.
Contact the TAs if you are having technical difficulties in 
submitting the assignment.

\item No code is needed for any of these problems. You can do the
calculations however you please. You need to turn in only the report. Please name 
your report as \texttt{$\langle$NetID$\rangle$-hw6.pdf}.

\end{itemize}
\end{footnotesize}



\begin{enumerate}
\item {\bf [Na\"ive Bayes and Learning Threshold Functions - 25 points]}

Consider the Boolean function $f_{TH(3,7)}$.  This is a threshold function
defined on the 7 dimensional Boolean cube as follows: given an instance $x$,
$f_{TH(3,7)}(x) = 1$ if and only if 3 or more of $x$'s components are 1.
\begin{enumerate}
\item {\bf [5 points]} Show that $f_{TH(3,7)}$ has a linear decision surface over the 
$7$ dimensional Boolean cube.
\item {\bf [10 points]}
Assume that you are given data sampled according to the uniform distribution
over the Boolean cube $\{0, 1\}^7$ and labeled according to $f_{TH(3,7)}$.
Use na\"ive Bayes to learn a hypothesis that predicts these labels.  What is
the hypothesis generated by the na\"ive Bayes algorithm?  (You may assume that
you have seen all the data required to get accurate estimates of the
probabilities).
\item {\bf [5 points]}
Show that the final hypothesis in (b) does not represent this function.
\item {\bf [5 points]}
Are the na\"ive Bayes assumptions satisfied by $f_{TH(3,7)}$?  Justify.
\end{enumerate}

\item {\bf [Na\"ive Bayes over Multinomial Distribution - 35 points]}

In this question, we will look into training a na\"ive Bayes classifier with a model that uses 
a multinomial distribution to represent documents. Assume that all the documents are 
written in a language which has only three words $a$, $b$, and $c$. All the 
documents have exactly $n$ words (each word can be either $a$, $b$, or $c$). We 
are given a labeled document collection $\{D_1, D_2, \ldots, D_m\}$. The label $y_i$ 
of document $D_i$ is $1$ or $0$, indicating whether $D_i$ is ``\texttt{good}'' or 
``\texttt{bad}''.

This model uses the multinominal distribution in the following way: Given the 
$i^{th}$ document $D_i$, we denote by $a_i$ (respectively, $b_i$, $c_i$) the 
number of times that word $a$ (respectively, $b$, $c$) appears in $D_i$. Therefore, 
$a_i + b_i + c_i = |D_i| = n$. We define
\[ \Pr(D_i | y = 1) = \frac{n!}{a_i! b_i! c_i!} \alpha_1^{a_i} \beta_1^{b_i} \gamma_1^{c_i} \]
where $\alpha_1$ (respectively, $\beta_1$, $\gamma_1$) is the probability that word 
$a$ (respectively, $b$, $c$) appears in a ``\texttt{good}'' document. Therefore,
$\alpha_1 + \beta_1 + \gamma_1 = 1$. Similarly,
\[ \Pr(D_i | y = 0) = \frac{n!}{a_i! b_i! c_i!} \alpha_0^{a_i} \beta_0^{b_i} \gamma_0^{c_i} \]
where $\alpha_0$ (respectively, $\beta_0$, $\gamma_0$) is the probability that word 
$a$ (respectively, $b$, $c$) appears in a ``\texttt{bad}'' document. Therefore, 
$\alpha_0 + \beta_0 + \gamma_0 = 1$.

\begin{enumerate}
\item Write down the expression for the log likelihood of the document $D_i$, 
$\log \Pr(D_i, y_i)$. 
\item Derive the expression for the maximum likelihood estimates for parameters $\alpha_1$, $\beta_1$, $\gamma_1$, 
$\alpha_0$, $\beta_0$, and $\gamma_0$.
\end{enumerate}

\textbf{Submission note:} You need not show the derivation of all six parameters separately. Some parameters are symmetric to others, and so, once you derive the expression for one, you can directly write down the expression for others. 

\textbf{Grading note: 10 points} for the derivation of one of the parameters, \textbf{5 points} each for the remaining five parameter expressions.

\item {\bf [Multivariate Poisson na\"ive Bayes - 30 points]}

In this question, we assume that the output and input variables are related via a Poisson distribution\footnote{http://en.wikipedia.org/wiki/Poisson\_distribution}.
You have two non-negative integer-valued numbers $X_1$ and $X_2$ as input-features and your class label $Y$ can take two values, $A$ and $B$.
Conditioned on $Y$, $X_i$ for $i=1,2$ follows a Poisson distribution with parameter $\lambda$ specific to the class label of $Y$. That is 
\[ Pr[X_i=x | Y = A] = \frac{e^{-\lambda^A_i} (\lambda^A_i)^x }{x!} ~~~\text{ and } ~~~ Pr[X_i=x | Y = B] = \frac{e^{-\lambda^B_i} (\lambda^B_i)^x }{x!} \text{ for } i =1,2 \]

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$X_1$ & $X_2$ & $Y$ \\
\hline
$0$ & $3$ & $A$ \\
$4$ & $8$ & $A$ \\
$2$ & $4$ & $A$ \\
$6$ & $2$ & $B$ \\
$3$ & $5$ & $B$ \\
$2$ & $1$ & $B$ \\
$5$ & $4$ & $B$ \\
\hline
\end{tabular}
\caption{Dataset for Poisson na\"ive Bayes}
\label{tab:poissonNBdata}
\end{center}
\end{table}

Assume that the given data in Table~\ref{tab:poissonNBdata} is generated by a Poisson na\"ive Bayes model. You will use this
 data to develop a na\"ive Bayes predictor over Poisson distribution.
\begin{table}[!h]
\begin{center}
\begin{tabular}{|rp{1in}|rp{1in}|}
\hline
$\Pr(Y\tight{=}A)=$ & & $\Pr(Y\tight{=}B)=$ & \\ \hline
$\lambda^A_1=$ & & $\lambda^B_1=$ & \\ \hline
$\lambda^A_2=$ & & $\lambda^B_2=$ & \\ \hline
\end{tabular}
\caption{Parameters for Poisson na\"ive Bayes}
\label{tab:poissonNBparams}
\end{center}
\end{table}

\begin{enumerate}
\item {\bf [10 points]} Compute the prior probabilities and parameter values i.e. fill out Table~\ref{tab:poissonNBparams}.
\item {\bf [10 points]} Based on the parameter values from 
Table~\ref{tab:poissonNBparams}, compute
\begin{equation*}
\frac{\Pr(X_1\tight{=}2 , X_2\tight{=}3 \loose{|} Y\tight{=}A)}{\Pr(X_1\tight{=}2 , X_2\tight{=}3 \loose{|} Y\tight{=}B)}
\end{equation*}
\item {\bf [10 points]} (5 points each for two questions)
Derive an algebraic expression for the Poisson na\"ive Bayes predictor for $Y$ in terms of the 
parameters estimated from the data.
% set (not specifically Table~\ref{tab:gaussianNBdata}). 
%\sout{Also write down how you will compute these probabilities.}

Use the parameters estimated from the data given in Table~\ref{tab:poissonNBdata} to 
create a Poisson na\"ive Bayes classifier. What will the classifier predict as the value of $Y$, given the data point:
$X_1\tight{=}2, X_2\tight{=}3$? 
\end{enumerate}



\item {\bf [Coin Toss - 10 points]}

In class, we discussed a scheme to generate a series of coin tosses as follows: For each element in the series, first a coin is tossed. If it comes up as a \texttt{T}, it is shown to the user. On the other hand, if the coin toss comes up as an \texttt{H}, then the coin is tossed the second time, and the outcome of this toss is shown to the user.

Assume that the probability of a coin toss coming up as an \texttt{H} is $p$ (and hence, the probability of it coming up as a \texttt{T} is $1-p$). Suppose you see a sequence \texttt{TTHTHHTHTT} generated based on the scheme given above.  What is the most likely value of $p$? (Assume a Bernoulli model to compute probability of the coin toss sequence.)
\end{enumerate}
\end{document}
